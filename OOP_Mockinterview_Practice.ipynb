{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOBcwSM0M7RHUS8zpsq4avW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kiana-M/CapsuleVAE/blob/master/OOP_Mockinterview_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s dive into a mock interview question that ties in data quality monitoring and OOP fundamentals.\n",
        "\n",
        "---\n",
        "\n",
        "**Interview Question:**\n",
        "\n",
        "\"Let's design a basic **Data Quality Monitoring System** in Python. This system should allow us to track data quality metrics for different data sources, such as completeness and uniqueness. We'll structure it to follow OOP principles, making it modular and extensible for future metrics.\"\n",
        "\n",
        "### Requirements\n",
        "\n",
        "1. **Abstract Base Class**: Create an abstract base class `DataQualityMetric`, which defines the blueprint for all metrics.\n",
        "   - This class should have an `evaluate` method that will be implemented in each concrete metric.\n",
        "\n",
        "2. **Concrete Metrics**: Define at least two concrete classes that inherit from `DataQualityMetric`:\n",
        "   - **CompletenessMetric**: Checks for missing values in a data source.\n",
        "   - **UniquenessMetric**: Checks for duplicate entries in a data source.\n",
        "   \n",
        "3. **DataSource Class**: A `DataSource` class represents the data to be monitored. It has a method to load data and another method to run metrics on it.\n",
        "\n",
        "4. **MetricEvaluator Class**: Finally, a class `MetricEvaluator` that takes in a `DataSource` and a list of `DataQualityMetric` objects, and evaluates each metric on the data source.\n",
        "\n",
        "**Assume we’re working with a pandas DataFrame as the data source.**\n",
        "\n",
        "---\n",
        "\n",
        "### Example Usage\n",
        "\n",
        "Here's a quick example of what this would look like when put together:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Define some data with missing values and duplicates for testing\n",
        "data = pd.DataFrame({\n",
        "    \"id\": [1, 2, 3, 4, None, 5, 2],\n",
        "    \"name\": [\"Alice\", \"Bob\", None, \"David\", \"Eve\", \"Frank\", \"Alice\"]\n",
        "})\n",
        "```\n",
        "\n",
        "Then, the code should work like this:\n",
        "\n",
        "```python\n",
        "source = DataSource(data)\n",
        "evaluator = MetricEvaluator(source, [CompletenessMetric(), UniquenessMetric()])\n",
        "results = evaluator.evaluate()\n",
        "print(results)\n",
        "```\n",
        "\n",
        "### Let’s Begin the Design\n",
        "\n",
        "1. **Step 1**: Start by defining the `DataQualityMetric` base class with the `evaluate` method.\n",
        "  \n",
        "2. **Step 2**: Implement `CompletenessMetric` and `UniquenessMetric` classes that inherit from `DataQualityMetric`.\n",
        "\n",
        "---\n",
        "\n",
        "Let’s start with **Step 1**: Go ahead and define the `DataQualityMetric` class as an abstract class, including the `evaluate` method.\n",
        "\n",
        "Let me know if you'd like me to help clarify any part or proceed to the next steps!"
      ],
      "metadata": {
        "id": "6kBFwIbVP1m_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MPDF785XPyA4"
      },
      "outputs": [],
      "source": [
        "\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class DataQualityMetric(ABC):\n",
        "\n",
        "    @abstractmethod\n",
        "    def evaluate(self):\n",
        "        raise NotImplementedError(\"Subclasses must implement this method.\")\n",
        "\n",
        "\n",
        "class CompletenessMetric(DataQualityMetric):\n",
        "    #Checks for missing values in a data source.\n",
        "\n",
        "    def evaluate(self, df):\n",
        "        if df.isnull().sum().sum() == 0:\n",
        "            return False\n",
        "        else:\n",
        "            return True\n",
        "\n",
        "\n",
        "class UniquenessMetric(DataQualityMetric):\n",
        "    #Checks for duplicate entries in a data source.\n",
        "\n",
        "    def evaluate(self, df):\n",
        "        if len(df) == len(df.drop_duplicates()):\n",
        "            return False\n",
        "        else:\n",
        "            return True\n",
        "\n",
        "class DataSource:\n",
        "    def __init__(self, data):\n",
        "        self.__data = data\n",
        "\n",
        "    @property\n",
        "    def load_data(self):\n",
        "        return self.__data\n",
        "\n",
        "\n",
        "class MetricEvaluator:\n",
        "    def __init__(self, source: DataSource, metric_list: list[DataQualityMetric]):\n",
        "        self.data = source\n",
        "        self.metric_list = metric_list\n",
        "\n",
        "    def run_metrics(self):\n",
        "        results = {}\n",
        "        if self.data.load_data.empty:\n",
        "            raise ValueError('No data')\n",
        "        for metric in self.metric_list:\n",
        "            metric_name = metric.__class__.__name__\n",
        "            results[metric_name] = metric.evaluate(self.data.load_data)\n",
        "        return results\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define some data with missing values and duplicates for testing\n",
        "data = pd.DataFrame({\n",
        "    \"id\": [1, 2, 3, 4, None, 5, 2],\n",
        "    \"name\": [\"Alice\", \"Bob\", None, \"David\", \"Eve\", \"Frank\", \"Alice\"]\n",
        "})\n"
      ],
      "metadata": {
        "id": "TCaNgDSpBFeN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source = DataSource(data)\n",
        "evaluator = MetricEvaluator(source, [CompletenessMetric(), UniquenessMetric()])\n",
        "results = evaluator.run_metrics()\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DqoBBttBMqq",
        "outputId": "692a0dc2-ae8b-4cd8-c043-e16daaed6a4f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'CompletenessMetric': True, 'UniquenessMetric': False}\n"
          ]
        }
      ]
    }
  ]
}